{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d22f5122",
   "metadata": {},
   "source": [
    "## Text preprocessing is a crucial step in Natural Language Processing (NLP) that involves cleaning and transforming raw text data into a format suitable for analysis and machine learning models. This process is vital for enhancing the performance and accuracy of NLP tasks.***\n",
    "\n",
    "***One key reason for text preprocessing is to remove noise and irrelevant information from the text, such as special characters, punctuation, and stop words. This helps in reducing the dimensionality of the data and improves the efficiency of subsequent analysis. Additionally, text normalization techniques, such as stemming and lemmatization, ensure that words are represented in their base or root form, reducing redundancy and enhancing the consistency of the dataset.***\n",
    "\n",
    "***For example, consider the sentence: \"The quick brown foxes are jumping over the lazy dogs.\" After preprocessing, it might become: \"quick brown fox jump lazy dog.\" This simplification facilitates better feature extraction and enables NLP models to focus on the essential linguistic elements.***\n",
    "\n",
    "Moreover, text preprocessing addresses issues like :\n",
    "- Lowercase letters.\n",
    "- Removing HTML tags.\n",
    "- Removing URLs.\n",
    "- Removing punctuation.\n",
    "- Chat Words Treatment.\n",
    "- Spelling Correction.\n",
    "- Removing stop words\n",
    "- Handling Emojies\n",
    "- Tokenization\n",
    "- Stemming\n",
    "- Lemmatization\n",
    "\n",
    "***We Will discuss the Solutions to Handles the above mentioned Issues.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4d0c35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Basis Libraries    \n",
    "import pandas as pd\n",
    "df = pd.read_csv('IMDB Dataset.csv')\n",
    "# Drop index column only if it exists\n",
    "if 'Index' in df.columns:\n",
    "    df.drop('Index', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0c1b8da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Probably my all-time favorite movie, a story o...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I sure would like to see a resurrection of a u...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>This show was an amazing, fresh &amp; innovative i...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Encouraged by the positive comments about this...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>If you like original gut wrenching laughter yo...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
       "5  Probably my all-time favorite movie, a story o...  positive\n",
       "6  I sure would like to see a resurrection of a u...  positive\n",
       "7  This show was an amazing, fresh & innovative i...  negative\n",
       "8  Encouraged by the positive comments about this...  negative\n",
       "9  If you like original gut wrenching laughter yo...  positive"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Head\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24e66eb",
   "metadata": {},
   "source": [
    "# 1. LowerCasing Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c878b5a7",
   "metadata": {},
   "source": [
    "***Lowercasing text in NLP preprocessing involves converting all letters in a text to lowercase. \n",
    "This step is essential for standardizing text data because it treats words with different cases (e.g., \"Word\" and \"word\") as the same, reducing \n",
    "vocabulary size and improving model efficiency. It ensures consistency in word representations, making it easier for algorithms to recognize patterns \n",
    "and associations. For example, \"The\" and \"the\" are treated as identical after lowercasing. This normalization simplifies subsequent processing steps, \n",
    "such as tokenization and feature extraction, leading to more accurate and robust NLP models.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd79cd5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Basically there's a family where a little boy (Jake) thinks there's a zombie in his closet & his parents are fighting all the time.<br /><br />This movie is slower than a soap opera... and suddenly, Jake decides to become Rambo and kill the zombie.<br /><br />OK, first of all when you're going to make a film you must Decide if its a thriller or a drama! As a drama the movie is watchable. Parents are divorcing & arguing like in real life. And then we have Jake with his closet which totally ruins all the film! I expected to see a BOOGEYMAN similar movie, and instead i watched a drama with some meaningless thriller spots.<br /><br />3 out of 10 just for the well playing parents & descent dialogs. As for the shots with Jake: just ignore them.\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pick any random Review \n",
    "df['review'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85eb6e84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"basically there's a family where a little boy (jake) thinks there's a zombie in his closet & his parents are fighting all the time.<br /><br />this movie is slower than a soap opera... and suddenly, jake decides to become rambo and kill the zombie.<br /><br />ok, first of all when you're going to make a film you must decide if its a thriller or a drama! as a drama the movie is watchable. parents are divorcing & arguing like in real life. and then we have jake with his closet which totally ruins all the film! i expected to see a boogeyman similar movie, and instead i watched a drama with some meaningless thriller spots.<br /><br />3 out of 10 just for the well playing parents & descent dialogs. as for the shots with jake: just ignore them.\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lower Casing the review\n",
    "df['review'][3].lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b721d2f6",
   "metadata": {},
   "source": [
    "#We can also Lowercase the Whole Corpus by using lower() function of Python.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ee1b9167",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a wonderful little production. &lt;br /&gt;&lt;br /&gt;the...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>petter mattei's \"love in the time of money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  one of the other reviewers has mentioned that ...  positive\n",
       "1  a wonderful little production. <br /><br />the...  positive\n",
       "2  i thought this was a wonderful way to spend ti...  positive\n",
       "3  basically there's a family where a little boy ...  negative\n",
       "4  petter mattei's \"love in the time of money\" is...  positive"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['review'] = df['review'].str.lower()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fece2e8",
   "metadata": {},
   "source": [
    "# 2. Remove HTML TAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d46ae59",
   "metadata": {},
   "source": [
    "***We can simply remove HTML tags by using the Regular Expressions.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "511a0d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Regular Expression\n",
    "import re\n",
    "\n",
    "# Function to remove HTML Tags\n",
    "def remove_html_tags(text):\n",
    "    pattern = re.compile('<.*?>')\n",
    "    return pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ccb0a4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<html><body><p> Movie 1</p><p> Actor - Aamir Khan</p><p> Click here to <a href='http://google.com'>download</a></p></body></html>\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Suppose we have a text Which Contains HTML Tags \n",
    "text = \"<html><body><p> Movie 1</p><p> Actor - Aamir Khan</p><p> Click here to <a href='http://google.com'>download</a></p></body></html>\"\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b61c3800",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Movie 1 Actor - Aamir Khan Click here to download'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply Function to Remove HTML Tags.\n",
    "remove_html_tags(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6479eb4d",
   "metadata": {},
   "source": [
    "#See How the Code perform well and clean the text from the HTML Tags , We can Also Apply this Function to Whole Corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "502097b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Function to Remove HTML Tags in our Dataset Colum Review.\n",
    "df['review'] = df['review'].apply(remove_html_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e17463",
   "metadata": {},
   "source": [
    "# 3. Remove URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0a44817a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here We also Use Regular Expressions to Remove URLs from Text or Whole Corpus.\n",
    "def remove_url(text):\n",
    "    pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4cdeb3",
   "metadata": {},
   "source": [
    "## Suppose we have the Followings Text With URL.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3e9c6ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = 'Check out my notebook https://www.kaggle.com/campusx/notebook8223fc1abb'\n",
    "text2 = 'Check out my notebook http://www.kaggle.com/campusx/notebook8223fc1abb'\n",
    "text3 = 'Google search here www.google.com'\n",
    "text4 = 'For notebook click https://www.kaggle.com/campusx/notebook8223fc1abb to search check www.google.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e4a19a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check out my notebook \n",
      "Check out my notebook \n",
      "Google search here \n",
      "For notebook click  to search check \n"
     ]
    }
   ],
   "source": [
    "# Lets Remove The URL by Calling Function\n",
    "print(remove_url(text1))\n",
    "print(remove_url(text2))\n",
    "print(remove_url(text3))\n",
    "print(remove_url(text4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617bfc39",
   "metadata": {},
   "source": [
    "# 4. Remove Punctuations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cccd4e7",
   "metadata": {},
   "source": [
    "Removing punctuation marks is essential in NLP text preprocessing to enhance the accuracy and efficiency of analysis. Punctuation marks like \n",
    "commas, periods, and quotation marks carry little semantic meaning and can introduce noise into the dataset. By removing them, the text becomes cleaner \n",
    "and more uniform, making it easier for machine learning models to extract meaningful features and patterns. Additionally, removing punctuation \n",
    "aids in standardizing the text, ensuring consistency across documents and improving the overall performance of NLP tasks such as sentiment analysis, \n",
    "text classification, and named entity recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e98243dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# From String we Imorts Punctuation.\n",
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f33f8ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing Punctuation in a Variable\n",
    "punc = string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e3170c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code defines a function, remove_punc1, that takes a text input and removes all punctuation characters from it using\n",
    "# the translate method with a translation table created by str.maketrans. This function effectively cleanses the text of punctuation symbols.\n",
    "def remove_punc(text):\n",
    "    return text.translate(str.maketrans('', '', punc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "73d63c0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The quick brown fox jumps over the lazy dog. However, the dog doesn't seem impressed! Oh no, it just yawned. How disappointing! Maybe a squirrel would elicit a reaction. Alas, the fox is out of luck.\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Text With Punctuation.\n",
    "text = \"The quick brown fox jumps over the lazy dog. However, the dog doesn't seem impressed! Oh no, it just yawned. How disappointing! Maybe a squirrel would elicit a reaction. Alas, the fox is out of luck.\"\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "35ef4b8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The quick brown fox jumps over the lazy dog However the dog doesnt seem impressed Oh no it just yawned How disappointing Maybe a squirrel would elicit a reaction Alas the fox is out of luck'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove Punctuation.\n",
    "remove_punc(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994d41b2",
   "metadata": {},
   "source": [
    "## Hence the function removes the punctuations from the text and we can also use this function to remove the punctuations from the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1e4e569b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "if you like original gut wrenching laughter you will like this movie. if you are young or old then you will love this movie, hell even my mom liked it.great camp!!!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'if you like original gut wrenching laughter you will like this movie if you are young or old then you will love this movie hell even my mom liked itgreat camp'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exmaple on whole Dataset.\n",
    "print(df['review'][9])\n",
    "\n",
    "# Remove Punctuation\n",
    "remove_punc(df['review'][9])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdaf6ad",
   "metadata": {},
   "source": [
    "# 5. Handling ChatWords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2196edcd",
   "metadata": {},
   "source": [
    "Handling ChatWords, also known as internet slang or informal language used in online communication, is important in NLP text preprocessing to ensure accurate analysis and understanding of text data. By converting ChatWords into their standard English equivalents or formal language equivalents, NLP models can effectively interpret the meaning of the text. This preprocessing step helps in maintaining consistency, improving the quality of input data, and enhancing the performance of NLP tasks such as sentiment analysis, chatbots, and information retrieval systems. Ultimately, handling ChatWords ensures better comprehension and more reliable results in NLP applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eb7b62ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here Come ChatWords Which i Get from a Github Repository\n",
    "# Repository Link : https://github.com/rishabhverma17/sms_slang_translator/blob/master/slang.txt\n",
    "chat_words = {\n",
    "    \"AFAIK\": \"As Far As I Know\",\n",
    "    \"AFK\": \"Away From Keyboard\",\n",
    "    \"ASAP\": \"As Soon As Possible\",\n",
    "    \"ATK\": \"At The Keyboard\",\n",
    "    \"ATM\": \"At The Moment\",\n",
    "    \"A3\": \"Anytime, Anywhere, Anyplace\",\n",
    "    \"BAK\": \"Back At Keyboard\",\n",
    "    \"BBL\": \"Be Back Later\",\n",
    "    \"BBS\": \"Be Back Soon\",\n",
    "    \"BFN\": \"Bye For Now\",\n",
    "    \"B4N\": \"Bye For Now\",\n",
    "    \"BRB\": \"Be Right Back\",\n",
    "    \"BRT\": \"Be Right There\",\n",
    "    \"BTW\": \"By The Way\",\n",
    "    \"B4\": \"Before\",\n",
    "    \"B4N\": \"Bye For Now\",\n",
    "    \"CU\": \"See You\",\n",
    "    \"CUL8R\": \"See You Later\",\n",
    "    \"CYA\": \"See You\",\n",
    "    \"FAQ\": \"Frequently Asked Questions\",\n",
    "    \"FC\": \"Fingers Crossed\",\n",
    "    \"FWIW\": \"For What It's Worth\",\n",
    "    \"FYI\": \"For Your Information\",\n",
    "    \"GAL\": \"Get A Life\",\n",
    "    \"GG\": \"Good Game\",\n",
    "    \"GN\": \"Good Night\",\n",
    "    \"GMTA\": \"Great Minds Think Alike\",\n",
    "    \"GR8\": \"Great!\",\n",
    "    \"G9\": \"Genius\",\n",
    "    \"IC\": \"I See\",\n",
    "    \"ICQ\": \"I Seek you (also a chat program)\",\n",
    "    \"ILU\": \"ILU: I Love You\",\n",
    "    \"IMHO\": \"In My Honest/Humble Opinion\",\n",
    "    \"IMO\": \"In My Opinion\",\n",
    "    \"IOW\": \"In Other Words\",\n",
    "    \"IRL\": \"In Real Life\",\n",
    "    \"KISS\": \"Keep It Simple, Stupid\",\n",
    "    \"LDR\": \"Long Distance Relationship\",\n",
    "    \"LMAO\": \"Laugh My A.. Off\",\n",
    "    \"LOL\": \"Laughing Out Loud\",\n",
    "    \"LTNS\": \"Long Time No See\",\n",
    "    \"L8R\": \"Later\",\n",
    "    \"MTE\": \"My Thoughts Exactly\",\n",
    "    \"M8\": \"Mate\",\n",
    "    \"NRN\": \"No Reply Necessary\",\n",
    "    \"OIC\": \"Oh I See\",\n",
    "    \"PITA\": \"Pain In The A..\",\n",
    "    \"PRT\": \"Party\",\n",
    "    \"PRW\": \"Parents Are Watching\",\n",
    "    \"QPSA?\": \"Que Pasa?\",\n",
    "    \"ROFL\": \"Rolling On The Floor Laughing\",\n",
    "    \"ROFLOL\": \"Rolling On The Floor Laughing Out Loud\",\n",
    "    \"ROTFLMAO\": \"Rolling On The Floor Laughing My A.. Off\",\n",
    "    \"SK8\": \"Skate\",\n",
    "    \"STATS\": \"Your sex and age\",\n",
    "    \"ASL\": \"Age, Sex, Location\",\n",
    "    \"THX\": \"Thank You\",\n",
    "    \"TTFN\": \"Ta-Ta For Now!\",\n",
    "    \"TTYL\": \"Talk To You Later\",\n",
    "    \"U\": \"You\",\n",
    "    \"U2\": \"You Too\",\n",
    "    \"U4E\": \"Yours For Ever\",\n",
    "    \"WB\": \"Welcome Back\",\n",
    "    \"WTF\": \"What The F...\",\n",
    "    \"WTG\": \"Way To Go!\",\n",
    "    \"WUF\": \"Where Are You From?\",\n",
    "    \"W8\": \"Wait...\",\n",
    "    \"7K\": \"Sick:-D Laugher\",\n",
    "    \"TFW\": \"That feeling when\",\n",
    "    \"MFW\": \"My face when\",\n",
    "    \"MRW\": \"My reaction when\",\n",
    "    \"IFYP\": \"I feel your pain\",\n",
    "    \"TNTL\": \"Trying not to laugh\",\n",
    "    \"JK\": \"Just kidding\",\n",
    "    \"IDC\": \"I don't care\",\n",
    "    \"ILY\": \"I love you\",\n",
    "    \"IMU\": \"I miss you\",\n",
    "    \"ADIH\": \"Another day in hell\",\n",
    "    \"ZZZ\": \"Sleeping, bored, tired\",\n",
    "    \"WYWH\": \"Wish you were here\",\n",
    "    \"TIME\": \"Tears in my eyes\",\n",
    "    \"BAE\": \"Before anyone else\",\n",
    "    \"FIMH\": \"Forever in my heart\",\n",
    "    \"BSAAW\": \"Big smile and a wink\",\n",
    "    \"BWL\": \"Bursting with laughter\",\n",
    "    \"BFF\": \"Best friends forever\",\n",
    "    \"CSL\": \"Can't stop laughing\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0db11f",
   "metadata": {},
   "source": [
    "## The code defines a function, chat_conversion, that replaces text with their corresponding chat acronyms from a predefined dictionary. It iterates through each word in the input text, checks if it exists in the dictionary, and replaces it if found. The modified text is then returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1dd5c4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function\n",
    "def chat_conversion(text):\n",
    "    new_text = []\n",
    "    for i in text.split():\n",
    "        if i.upper() in chat_words:\n",
    "            new_text.append(chat_words[i.upper()])\n",
    "        else:\n",
    "            new_text.append(i)\n",
    "    return \" \".join(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "93a576d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In My Honest/Humble Opinion he is the best\n",
      "For Your Information Islamabad is the capital of Pakistan\n"
     ]
    }
   ],
   "source": [
    "# Text\n",
    "text = 'IMHO he is the best'\n",
    "text1 = 'FYI Islamabad is the capital of Pakistan'\n",
    "# Calling function\n",
    "print(chat_conversion(text))\n",
    "print(chat_conversion(text1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6325bee",
   "metadata": {},
   "source": [
    "We can apply this passing review through the function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6482b0f3",
   "metadata": {},
   "source": [
    "# 6. Spelling Correction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a2698e",
   "metadata": {},
   "source": [
    "Spelling correction is a crucial aspect of NLP text preprocessing to enhance data quality and improve model performance. It addresses errors in text caused by typographical mistakes, irregularities, or variations in spelling. Correcting spelling errors ensures consistency and accuracy in the dataset, reducing ambiguity and improving the reliability of NLP tasks like sentiment analysis, machine translation, and information retrieval. By standardizing spelling across the dataset, models can better understand and process text, leading to more precise and reliable results in natural language processing applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5caec281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import this Library to Handle the Spelling Issue.\n",
    "import textblob\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b05c231e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ceertain conditionas duriing seveal ggenerations aree moodified in the saame maner.\n",
      "certain conditions during several generations are modified in the same manner.\n",
      "The cat sat on the cuchion. while plyaiing\n",
      "The cat sat on the cushion. while playing\n"
     ]
    }
   ],
   "source": [
    "# Incorrect text\n",
    "incorrect_text = 'ceertain conditionas duriing seveal ggenerations aree moodified in the saame maner.'\n",
    "print(incorrect_text)\n",
    "# Text 2 \n",
    "incorrect_text2 = 'The cat sat on the cuchion. while plyaiing'\n",
    "# Calling function\n",
    "textBlb = TextBlob(incorrect_text)\n",
    "textBlb1 = TextBlob(incorrect_text2)\n",
    "# Corrected Text\n",
    "print(textBlb.correct().string)\n",
    "print(incorrect_text2)\n",
    "print(textBlb1.correct().string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155d31a4",
   "metadata": {},
   "source": [
    "# 7. Handling StopWords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75476e81",
   "metadata": {},
   "source": [
    "In NLP text preprocessing, removing stop words is crucial to enhance the quality and efficiency of analysis. Stop words are common words like \"the,\" \"is,\" and \"and,\" which appear frequently in text but carry little semantic meaning. By eliminating stop words, we reduce noise in the data, decrease the dimensionality of the dataset, and improve the accuracy of NLP tasks such as sentiment analysis, topic modeling, and text classification. This process streamlines the analysis by focusing on the significant words that carry more meaningful information, leading to better model performance and interpretation of results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dbb6a221",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2acad2d9",
   "metadata": {},
   "source": [
    "Here we can see all the stopwords in English.However we can chose different Languages also like spanish etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "202a2287",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a1e7f4",
   "metadata": {},
   "source": [
    "The code defines a function, remove_stopwords, which removes stopwords from a given text. It iterates through each word in the text, checks if it is a stopword, and appends it to a new list if it is not. Then, it clears the original list, returns the modified text.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b1ae307a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function\n",
    "def remove_stopwords(text):\n",
    "    new_text = []\n",
    "    \n",
    "    for word in text.split():\n",
    "        if word in stopword:\n",
    "            new_text.append('')\n",
    "        else:\n",
    "            new_text.append(word)\n",
    "    x = new_text[:]\n",
    "    new_text.clear()\n",
    "    return \" \".join(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3e2d0089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text With Stop Words :probably my all-time favorite movie, a story of selflessness, sacrifice and dedication to a noble cause, but it's not preachy or boring. it just never gets old, despite my having seen it some 15 or more times\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'probably  all-time favorite movie,  story  selflessness, sacrifice  dedication   noble cause,    preachy  boring.   never gets old, despite   seen   15   times'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Text\n",
    "text = 'probably my all-time favorite movie, a story of selflessness, sacrifice and dedication to a noble cause, but it\\'s not preachy or boring. it just never gets old, despite my having seen it some 15 or more times'\n",
    "print(f'Text With Stop Words :{text}')\n",
    "# Calling Function\n",
    "remove_stopwords(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e65236",
   "metadata": {},
   "source": [
    "# We can Apply the same Function on Whole Corpus also "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "81db4508",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        one    reviewers  mentioned   watching  1 oz e...\n",
       "1         wonderful little production.  filming techniq...\n",
       "2         thought    wonderful way  spend time    hot s...\n",
       "3        basically there's  family   little boy (jake) ...\n",
       "4        petter mattei's \"love   time  money\"   visuall...\n",
       "                               ...                        \n",
       "49995     thought  movie    right good job.    creative...\n",
       "49996    bad plot, bad dialogue, bad acting, idiotic di...\n",
       "49997       catholic taught  parochial elementary schoo...\n",
       "49998     going    disagree   previous comment  side  m...\n",
       "49999     one expects  star trek movies   high art,   f...\n",
       "Name: review, Length: 50000, dtype: object"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['review'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc50d266",
   "metadata": {},
   "source": [
    "# 8. Handling Emojies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f80dfd1",
   "metadata": {},
   "source": [
    "Handling emojis in NLP text preprocessing is essential for several reasons. Emojis convey valuable information about sentiment, emotion, and context in text data, especially in informal communication channels like social media. However, they pose challenges for NLP algorithms due to their non-textual nature. Preprocessing involves converting emojis into meaningful representations, such as replacing them with textual descriptions or mapping them to specific sentiment categories. By handling emojis effectively, NLP models can accurately interpret and analyze text data, leading to improved performance in sentiment analysis, emotion detection, and other NLP tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d17c7d",
   "metadata": {},
   "source": [
    "### 8.1 Simply Remove Emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "09820ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again Here we use The Regular Expressions to Remove the Emojies from Text or Whole Corpus.\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7c88694c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loved the movie. It was ðŸ˜˜ \n",
      " Python is ðŸ”¥ \n",
      " Python is ðŸ˜Ž \n",
      " Python is ðŸ¤Ž \n",
      " Python is ðŸ˜¥ \n",
      " Python is ðŸ˜¶â€ðŸŒ«ï¸\n",
      "Loved the movie. It was \n",
      "Python is \n",
      "Python is \n",
      "Python is ðŸ¤Ž\n",
      "Python is \n",
      "Python is â€\n"
     ]
    }
   ],
   "source": [
    "# Texts \n",
    "text = \"Loved the movie. It was ðŸ˜˜\"\n",
    "text1 = 'Python is ðŸ”¥'\n",
    "text2 = 'Python is ðŸ˜Ž'\n",
    "text3 = 'Python is ðŸ¤Ž'\n",
    "text4 = 'Python is ðŸ˜¥'\n",
    "text5 = 'Python is ðŸ˜¶â€ðŸŒ«ï¸'\n",
    "print(text ,'\\n', text1,'\\n', text2,'\\n', text3,'\\n', text4,'\\n',text5)\n",
    "\n",
    "# Remove Emojies using Fucntion\n",
    "print(remove_emoji(text))\n",
    "print(remove_emoji(text1))\n",
    "print(remove_emoji(text2))\n",
    "print(remove_emoji(text3))\n",
    "print(remove_emoji(text4))\n",
    "print(remove_emoji(text5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f53efc2",
   "metadata": {},
   "source": [
    "### 8.2 Simply Convert Emojis into text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6d42153d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting emoji\n",
      "  Downloading emoji-2.15.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Downloading emoji-2.15.0-py3-none-any.whl (608 kB)\n",
      "   ---------------------------------------- 0.0/608.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/608.4 kB ? eta -:--:--\n",
      "   ---------------------------------- ----- 524.3/608.4 kB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 608.4/608.4 kB 2.0 MB/s eta 0:00:00\n",
      "Installing collected packages: emoji\n",
      "Successfully installed emoji-2.15.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "803ad742",
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0120d379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loved the movie. It was :face_blowing_a_kiss:\n",
      "Python is :fire:\n",
      "Python is :smiling_face_with_sunglasses:\n",
      "Python is :brown_heart:\n",
      "Python is :sad_but_relieved_face:\n",
      "Python is :face_in_clouds:\n"
     ]
    }
   ],
   "source": [
    "# Calling the Emoji tool Demojize.\n",
    "print(emoji.demojize(text))\n",
    "print(emoji.demojize(text1))\n",
    "print(emoji.demojize(text2))\n",
    "print(emoji.demojize(text3))\n",
    "print(emoji.demojize(text4))\n",
    "print(emoji.demojize(text5))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a8754f",
   "metadata": {},
   "source": [
    "# 9. Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642e8929",
   "metadata": {},
   "source": [
    "Tokenization is a crucial step in NLP text preprocessing where text is segmented into smaller units, typically words or subwords, known as tokens. This process is essential for several reasons. Firstly, it breaks down the text into manageable units for analysis and processing. Secondly, it standardizes the representation of words, enabling consistency in language modeling tasks. Additionally, tokenization forms the basis for feature extraction and modeling in NLP, facilitating tasks such as sentiment analysis, named entity recognition, and machine translation. Overall, tokenization plays a fundamental role in preparing text data for further analysis and modeling in NLP applications.\n",
    "\n",
    "We Generally do 2 Type of tokenization \n",
    "1. Word tokenization \n",
    "2. Sentence Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da19a48",
   "metadata": {},
   "source": [
    "### 9.1 NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a1178a",
   "metadata": {},
   "source": [
    "NLTK is a Library used to tokenize text into sentences and words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e0d8e08f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47063ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f96ca08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraray \n",
    "from nltk.tokenize import word_tokenize,sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "396fd997",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'going', 'to', 'visit', 'delhi', '!']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Text\n",
    "sentence = 'I am going to visit delhi!'\n",
    "# Calling tool\n",
    "word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a007977",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Lorem Ipsum is simply dummy text of the printing and typesetting industry?',\n",
       " \"Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, \\nwhen an unknown printer took a galley of type and scrambled it to make a type specimen book.\"]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Whole text Containing 2 or more Sentences\n",
    "text = \"\"\"Lorem Ipsum is simply dummy text of the printing and typesetting industry? \n",
    "Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, \n",
    "when an unknown printer took a galley of type and scrambled it to make a type specimen book.\"\"\"\n",
    "\n",
    "# Sentence Based Tokenization\n",
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4dbb1630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'have', 'a', 'Ph.D', 'in', 'A.I']\n",
      "['We', \"'re\", 'here', 'to', 'help', '!', 'mail', 'us', 'at', 'nks', '@', 'gmail.com']\n",
      "['A', '5km', 'ride', 'cost', '$', '10.50']\n"
     ]
    }
   ],
   "source": [
    "# Some Sentences \n",
    "sent5 = 'I have a Ph.D in A.I'\n",
    "sent6 = \"We're here to help! mail us at nks@gmail.com\"\n",
    "sent7 = 'A 5km ride cost $10.50'\n",
    "\n",
    "# Word Tokenize the Sentences\n",
    "print(word_tokenize(sent5))\n",
    "print(word_tokenize(sent6))\n",
    "print(word_tokenize(sent7))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20eaa245",
   "metadata": {},
   "source": [
    "### 9.1 Spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98497ca",
   "metadata": {},
   "source": [
    "Spacy is a Library used to tokenize text into sentences and words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17f97956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.8.11-cp312-cp312-win_amd64.whl.metadata (28 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Downloading murmurhash-1.0.15-cp312-cp312-win_amd64.whl.metadata (2.3 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Downloading cymem-2.0.13-cp312-cp312-win_amd64.whl.metadata (9.9 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Downloading preshed-3.0.12-cp312-cp312-win_amd64.whl.metadata (2.6 kB)\n",
      "Collecting thinc<8.4.0,>=8.3.4 (from spacy)\n",
      "  Downloading thinc-8.3.10-cp312-cp312-win_amd64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Downloading wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Downloading srsly-2.5.2-cp312-cp312-win_amd64.whl.metadata (20 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.5.0,>=0.4.2 (from spacy)\n",
      "  Downloading weasel-0.4.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting typer-slim<1.0.0,>=0.3.0 (from spacy)\n",
      "  Downloading typer_slim-0.21.1-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (1.26.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\asus\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (2.10.4)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\asus\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\asus\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (75.8.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (24.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\asus\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\asus\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\asus\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\asus\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asus\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.12.14)\n",
      "Collecting blis<1.4.0,>=1.3.0 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading blis-1.3.3-cp312-cp312-win_amd64.whl.metadata (7.7 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from typer-slim<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.4.2->spacy)\n",
      "  Downloading cloudpathlib-0.23.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting smart-open<8.0.0,>=5.2.1 (from weasel<0.5.0,>=0.4.2->spacy)\n",
      "  Downloading smart_open-7.5.0-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: wrapt in c:\\users\\asus\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.4.2->spacy) (1.17.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->spacy) (2.1.5)\n",
      "Downloading spacy-3.8.11-cp312-cp312-win_amd64.whl (14.2 MB)\n",
      "   ---------------------------------------- 0.0/14.2 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/14.2 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/14.2 MB 1.7 MB/s eta 0:00:09\n",
      "   --- ------------------------------------ 1.3/14.2 MB 2.6 MB/s eta 0:00:06\n",
      "   ------ --------------------------------- 2.4/14.2 MB 3.4 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 3.1/14.2 MB 3.4 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 4.5/14.2 MB 3.9 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 5.0/14.2 MB 3.8 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 5.8/14.2 MB 3.8 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 6.6/14.2 MB 3.8 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 7.3/14.2 MB 3.8 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 8.1/14.2 MB 3.8 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 8.9/14.2 MB 3.8 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 9.7/14.2 MB 3.7 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 10.2/14.2 MB 3.7 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 11.0/14.2 MB 3.7 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 11.8/14.2 MB 3.7 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 12.6/14.2 MB 3.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 13.4/14.2 MB 3.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  14.2/14.2 MB 3.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 14.2/14.2 MB 3.6 MB/s eta 0:00:00\n",
      "Downloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading cymem-2.0.13-cp312-cp312-win_amd64.whl (40 kB)\n",
      "Downloading murmurhash-1.0.15-cp312-cp312-win_amd64.whl (25 kB)\n",
      "Downloading preshed-3.0.12-cp312-cp312-win_amd64.whl (118 kB)\n",
      "Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Downloading srsly-2.5.2-cp312-cp312-win_amd64.whl (654 kB)\n",
      "   ---------------------------------------- 0.0/654.8 kB ? eta -:--:--\n",
      "   -------------------------------- ------- 524.3/654.8 kB 5.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 654.8/654.8 kB 3.6 MB/s eta 0:00:00\n",
      "Downloading thinc-8.3.10-cp312-cp312-win_amd64.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ------------------------ --------------- 1.0/1.7 MB 6.3 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 1.6/1.7 MB 4.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.7/1.7 MB 3.2 MB/s eta 0:00:00\n",
      "Downloading blis-1.3.3-cp312-cp312-win_amd64.whl (6.2 MB)\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   ---------- ----------------------------- 1.6/6.2 MB 7.0 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 2.6/6.2 MB 5.8 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 3.1/6.2 MB 5.3 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 3.9/6.2 MB 4.8 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 4.7/6.2 MB 4.5 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 5.5/6.2 MB 4.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.2/6.2 MB 4.2 MB/s eta 0:00:00\n",
      "Downloading confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Downloading typer_slim-0.21.1-py3-none-any.whl (47 kB)\n",
      "Downloading wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Downloading weasel-0.4.3-py3-none-any.whl (50 kB)\n",
      "Downloading cloudpathlib-0.23.0-py3-none-any.whl (62 kB)\n",
      "Downloading smart_open-7.5.0-py3-none-any.whl (63 kB)\n",
      "Installing collected packages: wasabi, spacy-loggers, spacy-legacy, smart-open, murmurhash, cymem, cloudpathlib, catalogue, blis, typer-slim, srsly, preshed, confection, weasel, thinc, spacy\n",
      "\n",
      "   ----------------------------------------  0/16 [wasabi]\n",
      "   ----------------------------------------  0/16 [wasabi]\n",
      "   -- -------------------------------------  1/16 [spacy-loggers]\n",
      "   ----- ----------------------------------  2/16 [spacy-legacy]\n",
      "   ----- ----------------------------------  2/16 [spacy-legacy]\n",
      "   ----- ----------------------------------  2/16 [spacy-legacy]\n",
      "   ------- --------------------------------  3/16 [smart-open]\n",
      "   ------- --------------------------------  3/16 [smart-open]\n",
      "   ------- --------------------------------  3/16 [smart-open]\n",
      "   ------- --------------------------------  3/16 [smart-open]\n",
      "   ------- --------------------------------  3/16 [smart-open]\n",
      "   ---------- -----------------------------  4/16 [murmurhash]\n",
      "   --------------- ------------------------  6/16 [cloudpathlib]\n",
      "   --------------- ------------------------  6/16 [cloudpathlib]\n",
      "   --------------- ------------------------  6/16 [cloudpathlib]\n",
      "   --------------- ------------------------  6/16 [cloudpathlib]\n",
      "   --------------- ------------------------  6/16 [cloudpathlib]\n",
      "   --------------- ------------------------  6/16 [cloudpathlib]\n",
      "   --------------- ------------------------  6/16 [cloudpathlib]\n",
      "   ----------------- ----------------------  7/16 [catalogue]\n",
      "   ----------------- ----------------------  7/16 [catalogue]\n",
      "   -------------------- -------------------  8/16 [blis]\n",
      "   ---------------------- -----------------  9/16 [typer-slim]\n",
      "   ---------------------- -----------------  9/16 [typer-slim]\n",
      "   ---------------------- -----------------  9/16 [typer-slim]\n",
      "   ------------------------- -------------- 10/16 [srsly]\n",
      "   ------------------------- -------------- 10/16 [srsly]\n",
      "   ------------------------- -------------- 10/16 [srsly]\n",
      "   ------------------------- -------------- 10/16 [srsly]\n",
      "   ------------------------- -------------- 10/16 [srsly]\n",
      "   ------------------------- -------------- 10/16 [srsly]\n",
      "   ------------------------- -------------- 10/16 [srsly]\n",
      "   ------------------------- -------------- 10/16 [srsly]\n",
      "   ------------------------- -------------- 10/16 [srsly]\n",
      "   ------------------------- -------------- 10/16 [srsly]\n",
      "   ------------------------- -------------- 10/16 [srsly]\n",
      "   ------------------------------ --------- 12/16 [confection]\n",
      "   -------------------------------- ------- 13/16 [weasel]\n",
      "   -------------------------------- ------- 13/16 [weasel]\n",
      "   -------------------------------- ------- 13/16 [weasel]\n",
      "   -------------------------------- ------- 13/16 [weasel]\n",
      "   ----------------------------------- ---- 14/16 [thinc]\n",
      "   ----------------------------------- ---- 14/16 [thinc]\n",
      "   ----------------------------------- ---- 14/16 [thinc]\n",
      "   ----------------------------------- ---- 14/16 [thinc]\n",
      "   ----------------------------------- ---- 14/16 [thinc]\n",
      "   ----------------------------------- ---- 14/16 [thinc]\n",
      "   ----------------------------------- ---- 14/16 [thinc]\n",
      "   ----------------------------------- ---- 14/16 [thinc]\n",
      "   ----------------------------------- ---- 14/16 [thinc]\n",
      "   ----------------------------------- ---- 14/16 [thinc]\n",
      "   ----------------------------------- ---- 14/16 [thinc]\n",
      "   ----------------------------------- ---- 14/16 [thinc]\n",
      "   ----------------------------------- ---- 14/16 [thinc]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ------------------------------------- -- 15/16 [spacy]\n",
      "   ---------------------------------------- 16/16 [spacy]\n",
      "\n",
      "Successfully installed blis-1.3.3 catalogue-2.0.10 cloudpathlib-0.23.0 confection-0.1.5 cymem-2.0.13 murmurhash-1.0.15 preshed-3.0.12 smart-open-7.5.0 spacy-3.8.11 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.5.2 thinc-8.3.10 typer-slim-0.21.1 wasabi-1.1.3 weasel-0.4.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install -U spacy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e2aaac",
   "metadata": {},
   "source": [
    "Run this in your terminal (not in Python):\n",
    "\n",
    "python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "77cb8301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code imports the Spacy library and loads the English language model 'en_core_web_sm' for natural language processing.\n",
    "# Pip install spacy library.\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2303f344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the Sentences in Words\n",
    "doc1 = nlp(sent5)\n",
    "doc2 = nlp(sent6)\n",
    "doc3 = nlp(sent7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "31748e27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We\n",
      "'re\n",
      "here\n",
      "to\n",
      "help\n",
      "!\n",
      "mail\n",
      "us\n",
      "at\n",
      "nks@gmail.com\n"
     ]
    }
   ],
   "source": [
    "# Print Token Genrated\n",
    "for token in doc2:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e5a8ec",
   "metadata": {},
   "source": [
    "# 10. Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1944e49a",
   "metadata": {},
   "source": [
    "Stemming is a text preprocessing technique in NLP used to reduce words to their root or base form, known as a stem, by removing suffixes. It helps in simplifying the vocabulary and reducing word variations, thereby improving the efficiency of downstream NLP tasks like information retrieval and sentiment analysis. By converting words to their common root, stemming increases the overlap between related words, enhancing the generalization ability of models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8344ce0a",
   "metadata": {},
   "source": [
    "Example ->\n",
    "\n",
    "running â†’ run\n",
    "\n",
    "runs    â†’ run\n",
    "\n",
    "runner  â†’ run\n",
    "\n",
    "played  â†’ play"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf882aa",
   "metadata": {},
   "source": [
    "Common Stemming Algorithms\n",
    "\n",
    "1. Porter Stemmer (most popular)\n",
    "\n",
    "2. Snowball Stemmer (improved Porter)\n",
    "3. Lancaster Stemmer (very aggressive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0509faaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n",
      "run\n",
      "runner\n",
      "studi\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "words = [\"running\", \"runs\", \"runner\", \"studies\"]\n",
    "\n",
    "for word in words:\n",
    "    print(stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "423b10b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Function Will Stem Words\n",
    "def stem_words(text):\n",
    "    return \" \".join([stemmer.stem(word) for word in text.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9d97cfc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probably my alltime favorite movie a story of selflessness sacrifice and dedication to a noble cause but its not preachy \n",
      "or boring it just never gets old despite my having seen it some 15 or more times in the last 25 years paul lukas performance brings\n",
      " tears to my eyes and bette davis in one of her very few truly sympathetic roles is a delight the kids are as grandma says more like \n",
      " dressedup midgets than children but that only makes them more fun to watch and the mothers slow awakening to whats happening in the \n",
      " world and under her own roof is believable and startling if i had a dozen thumbs theyd all be up for this movie\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'probabl my alltim favorit movi a stori of selfless sacrific and dedic to a nobl caus but it not preachi or bore it just never get old despit my have seen it some 15 or more time in the last 25 year paul luka perform bring tear to my eye and bett davi in one of her veri few truli sympathet role is a delight the kid are as grandma say more like dressedup midget than children but that onli make them more fun to watch and the mother slow awaken to what happen in the world and under her own roof is believ and startl if i had a dozen thumb theyd all be up for thi movi'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"probably my alltime favorite movie a story of selflessness sacrifice and dedication to a noble cause but its not preachy \n",
    "or boring it just never gets old despite my having seen it some 15 or more times in the last 25 years paul lukas performance brings\n",
    " tears to my eyes and bette davis in one of her very few truly sympathetic roles is a delight the kids are as grandma says more like \n",
    " dressedup midgets than children but that only makes them more fun to watch and the mothers slow awakening to whats happening in the \n",
    " world and under her own roof is believable and startling if i had a dozen thumbs theyd all be up for this movie\"\"\"\n",
    "print(text)\n",
    "\n",
    "# Calling Function\n",
    "stem_words(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269ffd41",
   "metadata": {},
   "source": [
    "### OR ->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c6ee37b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probabl\n",
      "my\n",
      "alltim\n",
      "favorit\n",
      "movi\n",
      "a\n",
      "stori\n",
      "of\n",
      "selfless\n",
      "sacrific\n",
      "and\n",
      "dedic\n",
      "to\n",
      "a\n",
      "nobl\n",
      "caus\n",
      "but\n",
      "it\n",
      "not\n",
      "preachi\n",
      "or\n",
      "bore\n",
      "it\n",
      "just\n",
      "never\n",
      "get\n",
      "old\n",
      "despit\n",
      "my\n",
      "have\n",
      "seen\n",
      "it\n",
      "some\n",
      "15\n",
      "or\n",
      "more\n",
      "time\n",
      "in\n",
      "the\n",
      "last\n",
      "25\n",
      "year\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "text = \"\"\"probably my alltime favorite movie a story of selflessness sacrifice and dedication to a noble cause but its not preachy \n",
    "or boring it just never gets old despite my having seen it some 15 or more times in the last 25 years\"\"\"\n",
    "\n",
    "words = text.split()   # split into words\n",
    "\n",
    "for word in words:\n",
    "    print(stemmer.stem(word))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579a3fb8",
   "metadata": {},
   "source": [
    "# 11. Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520bb836",
   "metadata": {},
   "source": [
    "Lemmatization is performed in NLP text preprocessing to reduce words to their base or dictionary form (lemma), enhancing consistency and simplifying analysis. Unlike stemming, which truncates words to their root form without considering meaning, lemmatization ensures that words are transformed to their canonical form, considering their part of speech. This process aids in reducing redundancy, improving text normalization, and enhancing the accuracy of downstream NLP tasks such as sentiment analysis, topic modeling, and information retrieval. Overall, lemmatization contributes to refining text data, facilitating more effective linguistic analysis and machine learning model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4707e242",
   "metadata": {},
   "source": [
    "ðŸ”¹ What Lemmatization Does\n",
    "\n",
    "It converts words to a valid English word.\n",
    "\n",
    "Examples :\n",
    "\n",
    "running  â†’ run\n",
    "\n",
    "better   â†’ good\n",
    "\n",
    "studies  â†’ study\n",
    "\n",
    "children â†’ child\n",
    "\n",
    "was      â†’ be\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d10b28",
   "metadata": {},
   "source": [
    "ðŸ”¹ How Lemmatization Works\n",
    "\n",
    "Lemmatization uses:\n",
    "\n",
    "Vocabulary (dictionary)\n",
    "\n",
    "Part-of-Speech (POS) tagging\n",
    "\n",
    "Because of this, it is slower but more accurate than stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7d6d9961",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\asus\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running\n",
      "better\n",
      "study\n",
      "child\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "words = [\"running\", \"better\", \"studies\", \"children\"]\n",
    "\n",
    "for word in words:\n",
    "    print(lemmatizer.lemmatize(word))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e8d85345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                Lemma               \n",
      "He                  He                  \n",
      "was                 be                  \n",
      "running             run                 \n",
      "and                 and                 \n",
      "eating              eat                 \n",
      "at                  at                  \n",
      "same                same                \n",
      "time                time                \n",
      "He                  He                  \n",
      "has                 have                \n",
      "bad                 bad                 \n",
      "habit               habit               \n",
      "of                  of                  \n",
      "swimming            swim                \n",
      "after               after               \n",
      "playing             play                \n",
      "long                long                \n",
      "hours               hours               \n",
      "in                  in                  \n",
      "the                 the                 \n",
      "Sun                 Sun                 \n"
     ]
    }
   ],
   "source": [
    "# Sentence \n",
    "sentence = \"He was running and eating at same time. He has bad habit of swimming after playing long hours in the Sun.\"\n",
    "\n",
    "# Intilize Punctuation\n",
    "punctuations=\"?:!.,;\"\n",
    "\n",
    "# Tokenize Word\n",
    "sentence_words = nltk.word_tokenize(sentence)\n",
    "\n",
    "# Using a Loop to Remove Punctuations.\n",
    "for word in sentence_words:\n",
    "    if word in punctuations:\n",
    "        sentence_words.remove(word)\n",
    "# Printing Word and Lemmatized Word\n",
    "print(\"{0:20}{1:20}\".format(\"Word\",\"Lemma\"))\n",
    "for word in sentence_words:\n",
    "    print (\"{0:20}{1:20}\".format(word,lemmatizer.lemmatize(word,pos='v')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad1f488",
   "metadata": {},
   "source": [
    "Well That's how the Lemmatizer Works.One Best Thing of Lemmatization is That, lemmatization ensures that words are transformed to their canonical form, considering their part of speech.However this Process is Slow"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
